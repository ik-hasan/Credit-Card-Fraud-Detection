# -*- coding: utf-8 -*-
"""CreditCardFraudDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15QW_5LIXROKatDaN6Da15hxTV-hiwequ
"""

# importing the libraries needed
import pandas as pd     #for importing data in code
import numpy as np      #used when we want to see our data in form of array
from sklearn.model_selection import train_test_split    #it is used to seperate train test data for using in model if we have only dataset not train and test seperately
from sklearn.preprocessing import StandardScaler    #for data preprocessing
from sklearn.linear_model import LogisticRegression   #here we used a model
from sklearn.tree import DecisionTreeClassifier     #here is also a model
from sklearn.ensemble import RandomForestClassifier     #here is also a model
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score     #they are some statistics related calculation libraries
import matplotlib.pyplot as plt      #for plotting graph and analyzing data in dataset
import seaborn as sns          #for plotting graph clearly

# loading the dataset
df = pd.read_csv('/content/fraudTrain.csv')    #importing data in this file
df1 = pd.read_csv('/content/fraudTest.csv')     #importin data in this file


# Explore the data
# print(df.head())
print(df['is_fraud'].value_counts())     #here we count number of unique entities and its quantity
print(df1['is_fraud'].value_counts())
print(df.columns)    #printing number of columns of df or data

# preprocessing
# check for missing values
print(df.isnull().sum())    #number of missing values in different columns
print(df1.isnull().sum())

# normalize the amount feature
# scaler = StandardScaler()

# from sklearn.preprocessing import StandardScaler

# Drop target and 'Time' column from features
features = df.drop(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state',
                    'zip', 'job', 'dob', 'trans_num', 'unix_time', 'is_fraud'], axis=1)
features1= df1.drop(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state',
                    'zip', 'job', 'dob', 'trans_num', 'unix_time', 'is_fraud'], axis=1)

#here just above we drop some columns and remaining columns are saved into features variable

print(features.shape[1])    #it is very basic
print(features)

# Apply StandardScaler to all features
scaler = StandardScaler()    #here we use some scaling techniques of columns that contained by features
features_scaled = scaler.fit_transform(features)
features_scaled1 = scaler.fit_transform(features1)

# Convert back to DataFrame
df_scaled = pd.DataFrame(features_scaled, columns=features.columns)          #here we save our featured scale data to df_scaled
df_scaled1 = pd.DataFrame(features_scaled1, columns=features1.columns)
# Add the columns back or add some columns that are nessasary that first we removed because we don't want scaling of these columns
df_scaled['Unnamed: 0'] = df['Unnamed: 0']

df_scaled['zip'] = df['zip']

df_scaled['unix_time'] = df['unix_time']
df_scaled['is_fraud'] = df['is_fraud']


df_scaled1['Unnamed: 0'] = df1['Unnamed: 0']

df_scaled1['zip'] = df1['zip']

df_scaled1['unix_time'] = df1['unix_time']
df_scaled1['is_fraud'] = df1['is_fraud']






# Split the data
X_train = df_scaled.drop('is_fraud', axis=1)   #here we drop is_fraud because by this we can make our data for training purpose
y_train = df_scaled['is_fraud']    #it is output for training purpose
X_test = df_scaled1.drop('is_fraud', axis=1)     #it is input for test purpose
y_test = df_scaled1['is_fraud']     #it is real output for test purpose

# print(X_train.shape, y_train.shape)
print(df_scaled.dtypes)   #here we want to see which column is of which datatype overall

lr = LogisticRegression(max_iter=1000)  #use of classifier
lr.fit(X_train, y_train)    #provide training data for classifier
y_pred_lr = lr.predict(X_test)    #provide test data for classifier

print("Logistic Regression:")
print(classification_report(y_test, y_pred_lr))      #final report of output accuracy of test data
